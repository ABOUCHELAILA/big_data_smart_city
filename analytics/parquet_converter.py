#!/usr/bin/env python3
"""
Conversion des donn√©es trait√©es au format Parquet
√âtape 5: Analytics Zone - Structuration analytique

Objectif: Convertir les donn√©es JSON trait√©es en format Parquet optimis√©
          pour l'analyse et les requ√™tes rapides.
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import argparse
import sys


class ParquetConverter:
    """Convertisseur de donn√©es JSON vers Parquet pour la zone analytics"""
    
    def __init__(self, hdfs_namenode: str = "hdfs://namenode:9000"):
        """
        Initialise le convertisseur
        
        Args:
            hdfs_namenode: URL du NameNode HDFS
        """
        self.hdfs_namenode = hdfs_namenode
        self.spark = self._create_spark_session()
    
    def _create_spark_session(self) -> SparkSession:
        """Cr√©e une session Spark configur√©e pour HDFS et Parquet"""
        return SparkSession.builder \
            .appName("SmartCity_Parquet_Converter") \
            .master("local[*]") \
            .config("spark.hadoop.fs.defaultFS", self.hdfs_namenode) \
            .config("spark.sql.parquet.compression.codec", "snappy") \
            .config("spark.sql.parquet.mergeSchema", "false") \
            .config("spark.sql.parquet.filterPushdown", "true") \
            .config("spark.driver.memory", "2g") \
            .getOrCreate()
    
    def convert_to_parquet(self, input_path: str, output_path: str, dataset_name: str):
        """
        Convertit un dataset JSON en Parquet
        
        Args:
            input_path: Chemin HDFS du fichier JSON source
            output_path: Chemin HDFS de base pour la sortie
            dataset_name: Nom du dataset
        """
        full_input = f"{self.hdfs_namenode}{input_path}/{dataset_name}"
        full_output = f"{self.hdfs_namenode}{output_path}/{dataset_name}"
        
        print(f"\nConversion: {dataset_name}")
        print(f"  Source: {full_input}")
        print(f"  Destination: {full_output}")
        
        try:
            # Lire le JSON
            df = self.spark.read.json(full_input)
            count = df.count()
            
            if count == 0:
                print(f"  ATTENTION: Aucune donn√©e dans {dataset_name}")
                return
            
            print(f"  Lignes lues: {count}")
            
            # Afficher le sch√©ma
            print(f"  Sch√©ma:")
            df.printSchema()
            
            # Sauvegarder en Parquet avec compression Snappy
            df.write.mode("overwrite") \
                .option("compression", "snappy") \
                .parquet(full_output)
            
            # V√©rifier la sauvegarde
            df_verify = self.spark.read.parquet(full_output)
            verify_count = df_verify.count()
            
            print(f"  ‚úì Conversion r√©ussie: {verify_count} lignes")
            
            # Afficher les statistiques du fichier Parquet
            print(f"  Format: Parquet avec compression Snappy")
            
        except Exception as e:
            print(f"  ‚úó Erreur lors de la conversion: {e}")
            import traceback
            traceback.print_exc()
    
    def convert_all(self, processed_path: str = "/data/processed/traffic",
                   analytics_path: str = "/data/analytics/traffic"):
        """
        Convertit tous les datasets trait√©s en Parquet
        
        Args:
            processed_path: Chemin des donn√©es trait√©es (JSON)
            analytics_path: Chemin de sortie analytics (Parquet)
        """
        print("=" * 80)
        print("CONVERSION VERS ZONE ANALYTICS - FORMAT PARQUET")
        print("=" * 80)
        
        datasets = [
            "zone_metrics",
            "road_metrics",
            "congestion_analysis",
            "hourly_patterns"
        ]
        
        success_count = 0
        
        for dataset in datasets:
            try:
                self.convert_to_parquet(processed_path, analytics_path, dataset)
                success_count += 1
            except Exception as e:
                print(f"Erreur pour {dataset}: {e}")
        
        print("\n" + "=" * 80)
        print(f"CONVERSION TERMIN√âE: {success_count}/{len(datasets)} datasets convertis")
        print("=" * 80)
        print(f"Donn√©es Parquet disponibles dans: {self.hdfs_namenode}{analytics_path}")
        
        # Justification du format Parquet
        print("\nüìä JUSTIFICATION DU FORMAT PARQUET:")
        print("  ‚Ä¢ Compression: R√©duction de 70-90% de la taille vs JSON")
        print("  ‚Ä¢ Performance: Lecture columnaire 10-100x plus rapide")
        print("  ‚Ä¢ Sch√©ma: Typage fort et validation automatique")
        print("  ‚Ä¢ Compatibilit√©: Support natif Spark, Hive, Presto, etc.")
        print("  ‚Ä¢ Optimisation: Predicate pushdown et column pruning")
        
        self.spark.stop()


def main():
    """Point d'entr√©e principal"""
    parser = argparse.ArgumentParser(
        description='Conversion des donn√©es trait√©es vers format Parquet'
    )
    parser.add_argument(
        '--hdfs-namenode',
        default='hdfs://namenode:9000',
        help='URL du NameNode HDFS'
    )
    parser.add_argument(
        '--processed-path',
        default='/data/processed/traffic',
        help='Chemin HDFS des donn√©es trait√©es (JSON)'
    )
    parser.add_argument(
        '--analytics-path',
        default='/data/analytics/traffic',
        help='Chemin HDFS de sortie analytics (Parquet)'
    )
    
    args = parser.parse_args()
    
    # Cr√©er et ex√©cuter le convertisseur
    converter = ParquetConverter(hdfs_namenode=args.hdfs_namenode)
    converter.convert_all(
        processed_path=args.processed_path,
        analytics_path=args.analytics_path
    )


if __name__ == '__main__':
    main()
