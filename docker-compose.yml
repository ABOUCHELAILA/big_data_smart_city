version: '3.8'

services:
  # --- MESSAGERIE : KAFKA ---
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092

  # --- STOCKAGE : HDFS ---
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    depends_on:
      - namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    volumes:
      - hadoop_datanode:/hadoop/dfs/data

  # --- ORCHESTRATION : AIRFLOW ---
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow

  airflow-init:
    image: apache/airflow:2.7.3
    user: root
    container_name: airflow-init
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'big-data-smart-city-123456789012345='
    command: version

  airflow-webserver:
    image: apache/airflow:2.7.3
    user: root
    container_name: airflow-webserver
    depends_on:
      - postgres
    ports:
      - "8081:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'big-data-smart-city-123456789012345='
      AIRFLOW__WEBSERVER__SECRET_KEY: 'super-secret-key-123'
    volumes: &airflow_volumes
      - ./airflow_dags:/opt/airflow/dags
      - ./data_generator:/opt/airflow/data_generator
      - ./kafka_producer:/opt/airflow/kafka_producer
      - ./kafka_consumer:/opt/airflow/kafka_consumer
      - ./spark_processing:/opt/airflow/spark_processing
      - ./analytics:/opt/airflow/analytics
      - ./scripts:/opt/airflow/scripts
      - ./logs:/opt/airflow/logs
    command: |
      bash -c "
      airflow db init
      airflow users create --username airflow --password airflow --firstname Admin --lastname User --role Admin --email admin@example.com
      chown -R airflow: /opt/airflow
      airflow webserver
      "

  airflow-scheduler:
    image: apache/airflow:2.7.3
    user: root
    container_name: airflow-scheduler
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'big-data-smart-city-123456789012345='
      AIRFLOW__WEBSERVER__SECRET_KEY: 'super-secret-key-123'
      PIP_ROOT_USER_ACTION: ignore
    volumes: *airflow_volumes
    command: |
      bash -c "
      apt-get update && apt-get install -y openjdk-11-jre-headless
      python3 -m pip install kafka-python==2.0.2 requests pyspark==3.4.1 pandas pyarrow
      chown -R airflow: /opt/airflow
      sleep 10
      exec airflow scheduler
      "

  # --- VISU & API ---
  data-api:
    image: python:3.9-slim
    container_name: data-api
    ports:
      - "5000:5000"
    working_dir: /opt/api
    volumes:
      - ./api:/opt/api
    command: bash -c "pip install flask flask-cors requests && python data_api.py"
    environment:
      - HDFS_NAMENODE=http://namenode:9870

  grafana:
    image: grafana/grafana:9.5.15
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=grafana-simple-json-datasource
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=grafana-simple-json-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards

volumes:
  hadoop_namenode:
  hadoop_datanode:
  grafana_data: